# pyright: reportUnknownVariableType=false, reportPrivateImportUsage=false, reportAny=false, reportUnknownMemberType=false

from __future__ import annotations

import argparse
import atexit
import io
import json
import os
import re
import textwrap
import threading
import time
import typing as t
import uuid
from collections import ChainMap, OrderedDict, defaultdict, deque
from collections.abc import Iterable, Iterator
from concurrent.futures import FIRST_EXCEPTION, Future, ThreadPoolExecutor, wait
from dataclasses import dataclass, field
from datetime import datetime, timezone
from functools import lru_cache, partial
from itertools import chain
from pathlib import Path
from threading import get_ident
from types import MappingProxyType

import dbt.flags as dbt_flags
import dbt.utils as dbt_utils
import pluggy
import ruamel.yaml
from agate.table import Table  # pyright: ignore[reportMissingTypeStubs]
from dbt.adapters.base.column import Column as BaseColumn
from dbt.adapters.base.impl import BaseAdapter
from dbt.adapters.base.relation import BaseRelation
from dbt.adapters.contracts.connection import AdapterResponse
from dbt.adapters.contracts.relation import RelationConfig
from dbt.adapters.factory import get_adapter, register_adapter
from dbt.config.runtime import RuntimeConfig
from dbt.context.providers import generate_runtime_macro_context
from dbt.contracts.graph.manifest import Manifest
from dbt.contracts.graph.nodes import (
    ColumnInfo,
    ManifestSQLNode,
    ModelNode,
    ResultNode,
    SeedNode,
    SourceDefinition,
)
from dbt.contracts.results import CatalogArtifact, CatalogResults, ColumnMetadata
from dbt.contracts.results import (
    CatalogKey as TableRef,
)
from dbt.mp_context import get_mp_context
from dbt.node_types import NodeType
from dbt.parser.manifest import ManifestLoader, process_node
from dbt.parser.models import ModelParser
from dbt.parser.sql import SqlBlockParser, SqlMacroParser
from dbt.task.docs.generate import Catalog
from dbt.task.sql import SqlCompileRunner
from dbt.tracking import disable_tracking
from dbt_common.clients.system import get_env
from dbt_common.context import set_invocation_context

import dbt_osmosis.core.logger as logger

__all__ = [
    "discover_project_dir",
    "discover_profiles_dir",
    "DbtConfiguration",
    "DbtProjectContext",
    "create_dbt_project_context",
    "create_yaml_instance",
    "YamlRefactorSettings",
    "YamlRefactorContext",
    "compile_sql_code",
    "execute_sql_code",
    "normalize_column_name",
    "get_table_ref",
    "get_columns",
    "create_missing_source_yamls",
    "get_current_yaml_path",
    "get_target_yaml_path",
    "build_yaml_file_mapping",
    "commit_yamls",
    "draft_restructure_delta_plan",
    "pretty_print_plan",
    "sync_node_to_yaml",
    "apply_restructure_plan",
    "inherit_upstream_column_knowledge",
    "inject_missing_columns",
    "remove_columns_not_in_database",
    "sort_columns_as_in_database",
    "sort_columns_alphabetically",
    "sort_columns_as_configured",
    "synchronize_data_types",
]

disable_tracking()

T = t.TypeVar("T")

EMPTY_STRING = ""
"""A null string constant for use in placeholder lists, this is always considered undocumented"""


# Basic DBT Setup
# ===============


def discover_project_dir() -> str:
    """Return the directory containing a dbt_project.yml if found, else the current dir. Checks DBT_PROJECT_DIR first if set."""
    if "DBT_PROJECT_DIR" in os.environ:
        project_dir = Path(os.environ["DBT_PROJECT_DIR"])
        if project_dir.is_dir():
            logger.info(":mag: DBT_PROJECT_DIR detected => %s", project_dir)
            return str(project_dir.resolve())
        logger.warning(":warning: DBT_PROJECT_DIR %s is not a valid directory.", project_dir)
    cwd = Path.cwd()
    for p in [cwd] + list(cwd.parents):
        if (p / "dbt_project.yml").exists():
            logger.info(":mag: Found dbt_project.yml at => %s", p)
            return str(p.resolve())
    logger.info(":mag: Defaulting to current directory => %s", cwd)
    return str(cwd.resolve())


def discover_profiles_dir() -> str:
    """Return the directory containing a profiles.yml if found, else ~/.dbt. Checks DBT_PROFILES_DIR first if set."""
    if "DBT_PROFILES_DIR" in os.environ:
        profiles_dir = Path(os.environ["DBT_PROFILES_DIR"])
        if profiles_dir.is_dir():
            logger.info(":mag: DBT_PROFILES_DIR detected => %s", profiles_dir)
            return str(profiles_dir.resolve())
        logger.warning(":warning: DBT_PROFILES_DIR %s is not a valid directory.", profiles_dir)
    if (Path.cwd() / "profiles.yml").exists():
        logger.info(":mag: Found profiles.yml in current directory.")
        return str(Path.cwd().resolve())
    home_profiles = str(Path.home() / ".dbt")
    logger.info(":mag: Defaulting to => %s", home_profiles)
    return home_profiles


@dataclass
class DbtConfiguration:
    """Configuration for a dbt project."""

    project_dir: str = field(default_factory=discover_project_dir)
    profiles_dir: str = field(default_factory=discover_profiles_dir)
    target: str | None = None
    profile: str | None = None
    threads: int | None = None
    single_threaded: bool | None = None
    vars: dict[str, t.Any] = field(default_factory=dict)
    quiet: bool = True
    disable_introspection: bool = False  # Internal

    def __post_init__(self) -> None:
        logger.debug(":bookmark_tabs: Setting invocation context with environment variables.")
        set_invocation_context(get_env())
        if self.threads and self.threads > 1:
            self.single_threaded = False


def config_to_namespace(cfg: DbtConfiguration) -> argparse.Namespace:
    """Convert a DbtConfiguration into a dbt-friendly argparse.Namespace."""
    logger.debug(":blue_book: Converting DbtConfiguration to argparse.Namespace => %s", cfg)
    ns = argparse.Namespace(
        project_dir=cfg.project_dir,
        profiles_dir=cfg.profiles_dir,
        target=cfg.target or os.getenv("DBT_TARGET"),
        profile=cfg.profile or os.getenv("DBT_PROFILE"),
        threads=cfg.threads,
        single_threaded=cfg.single_threaded,
        vars=cfg.vars,
        which="parse",
        quiet=cfg.quiet,
        DEBUG=False,
        REQUIRE_RESOURCE_NAMES_WITHOUT_SPACES=False,
    )
    return ns


@dataclass
class DbtProjectContext:
    """A data object that includes references to:

    - The loaded dbt config
    - The manifest
    - The sql/macro parsers

    With mutexes for thread safety. The adapter is lazily instantiated and has a TTL which allows
    for re-use across multiple operations in long-running processes. (is the idea)
    """

    config: DbtConfiguration
    """The configuration for the dbt project used to initialize the runtime cfg and manifest"""
    runtime_cfg: RuntimeConfig
    """The dbt project runtime config associated with the context"""
    manifest: Manifest
    """The dbt project manifest"""
    sql_parser: SqlBlockParser
    """Parser for dbt Jinja SQL blocks"""
    macro_parser: SqlMacroParser
    """Parser for dbt Jinja macros"""
    connection_ttl: float = 3600.0
    """Max time in seconds to keep a db connection alive before recycling it, mostly useful for very long runs"""

    _adapter_mutex: threading.Lock = field(default_factory=threading.Lock, init=False)
    _manifest_mutex: threading.Lock = field(default_factory=threading.Lock, init=False)
    _adapter: BaseAdapter | None = field(default=None, init=False)
    _connection_created_at: dict[int, float] = field(default_factory=dict, init=False)

    @property
    def is_connection_expired(self) -> bool:
        """Check if the adapter has expired based on the adapter TTL."""
        expired = (
            time.time() - self._connection_created_at.setdefault(get_ident(), 0.0)
            > self.connection_ttl
        )
        logger.debug(":hourglass_flowing_sand: Checking if connection is expired => %s", expired)
        return expired

    @property
    def adapter(self) -> BaseAdapter:
        """Get the adapter instance, creating a new one if the current one has expired."""
        with self._adapter_mutex:
            if not self._adapter:
                logger.info(":wrench: Instantiating new adapter because none is currently set.")
                adapter = _instantiate_adapter(self.runtime_cfg)
                adapter.set_macro_resolver(self.manifest)
                _ = adapter.acquire_connection()
                self._adapter = adapter
                self._connection_created_at[get_ident()] = time.time()
                logger.info(
                    ":wrench: Successfully acquired new adapter connection for thread => %s",
                    get_ident(),
                )
            elif self.is_connection_expired:
                logger.info(
                    ":wrench: Refreshing db connection for thread => %s",
                    get_ident(),
                )
                self._adapter.connections.release()
                self._adapter.connections.clear_thread_connection()
                _ = self._adapter.acquire_connection()
                self._connection_created_at[get_ident()] = time.time()
        return self._adapter

    @property
    def manifest_mutex(self) -> threading.Lock:
        """Return the manifest mutex for thread safety."""
        return self._manifest_mutex


def _add_cross_project_references(manifest, dbt_loom, project_name):
    """Add cross-project references to the dbt manifest from dbt-loom defined manifests."""
    loomnodes = []
    loom = dbt_loom.dbtLoom(project_name)
    loom_manifests = loom.manifests
    logger.info(":arrows_counterclockwise: Loaded dbt loom manifests!")
    for name, loom_manifest in loom_manifests.items():
        if loom_manifest.get("nodes"):
            loom_manifest_nodes = loom_manifest.get("nodes")
            for _, node in loom_manifest_nodes.items():
                if node.get("access"):
                    node_access = node.get("access")
                    if node_access != "protected":
                        if node.get("resource_type") == "model":
                            loomnodes.append(ModelParser.parse_from_dict(None, node))
        for node in loomnodes:
            manifest.nodes[node.unique_id] = node
        logger.info(
            f":arrows_counterclockwise: added {len(loomnodes)} exposed nodes from {name} to the dbt manifest!"
        )
    return manifest


def _instantiate_adapter(runtime_config: RuntimeConfig) -> BaseAdapter:
    """Instantiate a dbt adapter based on the runtime configuration."""
    logger.debug(":mag: Registering adapter for runtime config => %s", runtime_config)
    adapter = get_adapter(runtime_config)
    adapter.set_macro_context_generator(t.cast(t.Any, generate_runtime_macro_context))
    adapter.connections.set_connection_name("dbt-osmosis")
    logger.debug(":hammer_and_wrench: Adapter instantiated => %s", adapter)
    return t.cast(BaseAdapter, t.cast(t.Any, adapter))


def create_dbt_project_context(config: DbtConfiguration) -> DbtProjectContext:
    """Build a DbtProjectContext from a DbtConfiguration."""
    logger.info(":wave: Creating DBT project context using config => %s", config)
    args = config_to_namespace(config)
    dbt_flags.set_from_args(args, args)
    runtime_cfg = RuntimeConfig.from_args(args)

    logger.info(":bookmark_tabs: Registering adapter as part of project context creation.")
    register_adapter(runtime_cfg, get_mp_context())

    loader = ManifestLoader(
        runtime_cfg,
        runtime_cfg.load_dependencies(),
    )
    manifest = loader.load()

    # check if dbt-loom is installed
    loom_imported = False
    try:
        dbt_loom = __import__("dbt_loom")
        loom_imported = True
    except ImportError:
        pass

    if loom_imported:
        manifest = _add_cross_project_references(manifest, dbt_loom, runtime_cfg.project_name)

    manifest.build_flat_graph()
    logger.info(":arrows_counterclockwise: Loaded the dbt project manifest!")

    if not config.disable_introspection:
        adapter = _instantiate_adapter(runtime_cfg)
        setattr(runtime_cfg, "adapter", adapter)
        adapter.set_macro_resolver(manifest)

    sql_parser = SqlBlockParser(runtime_cfg, manifest, runtime_cfg)
    macro_parser = SqlMacroParser(runtime_cfg, manifest)

    logger.info(":sparkles: DbtProjectContext successfully created!")
    return DbtProjectContext(
        config=config,
        runtime_cfg=runtime_cfg,
        manifest=manifest,
        sql_parser=sql_parser,
        macro_parser=macro_parser,
    )


def _reload_manifest(context: DbtProjectContext) -> None:
    """Reload the dbt project manifest. Useful for picking up mutations."""
    logger.info(":arrows_counterclockwise: Reloading the dbt project manifest!")
    loader = ManifestLoader(context.runtime_cfg, context.runtime_cfg.load_dependencies())
    manifest = loader.load()
    manifest.build_flat_graph()
    if not context.config.disable_introspection:
        context.adapter.set_macro_resolver(manifest)
    context.manifest = manifest
    logger.info(":white_check_mark: Manifest reloaded => %s", context.manifest.metadata)


# YAML + File Data
# ================


def create_yaml_instance(
    indent_mapping: int = 2,
    indent_sequence: int = 4,
    indent_offset: int = 2,
    width: int = 100,
    preserve_quotes: bool = False,
    default_flow_style: bool = False,
    encoding: str = "utf-8",
) -> ruamel.yaml.YAML:
    """Returns a ruamel.yaml.YAML instance configured with the provided settings."""
    logger.debug(":notebook: Creating ruamel.yaml.YAML instance with custom formatting.")
    y = ruamel.yaml.YAML()
    y.indent(mapping=indent_mapping, sequence=indent_sequence, offset=indent_offset)
    y.width = width
    y.preserve_quotes = preserve_quotes
    y.default_flow_style = default_flow_style
    y.encoding = encoding

    def str_representer(dumper: ruamel.yaml.RoundTripDumper, data: str) -> t.Any:
        newlines = len(data.splitlines())
        if newlines == 1 and len(data) > width - len(f"description{y.prefix_colon}: "):
            return dumper.represent_scalar("tag:yaml.org,2002:str", data, style=">")
        if newlines > 1:
            return dumper.represent_scalar("tag:yaml.org,2002:str", data, style="|")
        return dumper.represent_scalar("tag:yaml.org,2002:str", data)

    y.representer.add_representer(str, str_representer)

    logger.debug(":notebook: YAML instance created => %s", y)
    return y


@dataclass
class SchemaFileLocation:
    """Describes the current and target location of a schema file."""

    target: Path
    current: Path | None = None
    node_type: NodeType = NodeType.Model

    @property
    def is_valid(self) -> bool:
        """Check if the current and target locations are valid."""
        valid = self.current == self.target
        logger.debug(":white_check_mark: Checking if schema file location is valid => %s", valid)
        return valid


@dataclass
class SchemaFileMigration:
    """Describes a schema file migration operation."""

    output: dict[str, t.Any] = field(
        default_factory=lambda: {"version": 2, "models": [], "sources": []}
    )
    supersede: dict[Path, list[ResultNode]] = field(default_factory=dict)


@dataclass
class RestructureOperation:
    """Represents a single operation to perform on a YAML file.

    This might be CREATE, UPDATE, SUPERSEDE, etc. In a more advanced approach,
    we might unify multiple steps under a single operation with sub-operations.
    """

    file_path: Path
    content: dict[str, t.Any]
    superseded_paths: dict[Path, list[ResultNode]] = field(default_factory=dict)


@dataclass
class RestructureDeltaPlan:
    """Stores all the operations needed to restructure the project."""

    operations: list[RestructureOperation] = field(default_factory=list)


@dataclass
class YamlRefactorSettings:
    """Settings for yaml based refactoring operations."""

    fqn: list[str] = field(default_factory=list)
    """Filter models to action via a fully qualified name match such as returned by `dbt ls`."""
    models: list[Path | str] = field(default_factory=list)
    """Filter models to action via a file path match."""
    dry_run: bool = False
    """Do not write changes to disk."""
    skip_merge_meta: bool = False
    """Skip merging upstream meta fields in the yaml files."""
    skip_add_columns: bool = False
    """Skip adding missing columns in the yaml files."""
    skip_add_tags: bool = False
    """Skip appending upstream tags in the yaml files."""
    skip_add_data_types: bool = False
    """Skip adding data types in the yaml files."""
    skip_add_source_columns: bool = False
    """Skip adding columns in the source yaml files specifically."""
    add_progenitor_to_meta: bool = False
    """Add a custom progenitor field to the meta section indicating a column's origin."""
    numeric_precision_and_scale: bool = False
    """Include numeric precision in the data type."""
    string_length: bool = False
    """Include character length in the data type."""
    force_inherit_descriptions: bool = False
    """Force inheritance of descriptions from upstream models, even if node has a valid description."""
    use_unrendered_descriptions: bool = False
    """Use unrendered descriptions preserving things like {{ doc(...) }} which are otherwise pre-rendered in the manifest object"""
    add_inheritance_for_specified_keys: list[str] = field(default_factory=list)
    """Include additional keys in the inheritance process."""
    output_to_lower: bool = False
    """Force column name and data type output to lowercase in the yaml files."""
    catalog_path: str | None = None
    """Path to the dbt catalog.json file to use preferentially instead of live warehouse introspection"""
    create_catalog_if_not_exists: bool = False
    """Generate the catalog.json for the project if it doesn't exist and use it for introspective queries."""


@dataclass
class YamlRefactorContext:
    """A data object that includes references to:

    - The dbt project context
    - The yaml refactor settings
    - A thread pool executor
    - A ruamel.yaml instance
    - A tuple of placeholder strings
    - The mutation count incremented during refactoring operations
    """

    project: DbtProjectContext
    settings: YamlRefactorSettings = field(default_factory=YamlRefactorSettings)
    pool: ThreadPoolExecutor = field(default_factory=ThreadPoolExecutor)
    yaml_handler: ruamel.yaml.YAML = field(default_factory=create_yaml_instance)
    yaml_handler_lock: threading.Lock = field(default_factory=threading.Lock)

    placeholders: tuple[str, ...] = (
        EMPTY_STRING,
        "Pending further documentation",
        "No description for this column",
        "Not documented",
        "Undefined",
    )

    _mutation_count: int = field(default=0, init=False)
    _catalog: CatalogResults | None = field(default=None, init=False)

    def register_mutations(self, count: int) -> None:
        """Increment the mutation count by a specified amount."""
        logger.debug(
            ":sparkles: Registering %s new mutations. Current count => %s",
            count,
            self._mutation_count,
        )
        self._mutation_count += count

    @property
    def mutation_count(self) -> int:
        """Read only property to access the mutation count."""
        return self._mutation_count

    @property
    def mutated(self) -> bool:
        """Check if the context has performed any mutations."""
        has_mutated = self._mutation_count > 0
        logger.debug(":white_check_mark: Has the context mutated anything? => %s", has_mutated)
        return has_mutated

    @property
    def source_definitions(self) -> dict[str, t.Any]:
        """The source definitions from the dbt project config."""
        c = self.project.runtime_cfg.vars.to_dict()
        toplevel_conf = _find_first(
            [c.get(k, {}) for k in ["dbt-osmosis", "dbt_osmosis"]], lambda v: bool(v), {}
        )
        return toplevel_conf.get("sources", {})

    @property
    def ignore_patterns(self) -> list[str]:
        """The column name ignore patterns from the dbt project config."""
        c = self.project.runtime_cfg.vars.to_dict()
        toplevel_conf = _find_first(
            [c.get(k, {}) for k in ["dbt-osmosis", "dbt_osmosis"]], lambda v: bool(v), {}
        )
        return toplevel_conf.get("column_ignore_patterns", [])

    @property
    def yaml_settings(self) -> dict[str, t.Any]:
        """The column name ignore patterns from the dbt project config."""
        c = self.project.runtime_cfg.vars.to_dict()
        toplevel_conf = _find_first(
            [c.get(k, {}) for k in ["dbt-osmosis", "dbt_osmosis"]], lambda v: bool(v), {}
        )
        return toplevel_conf.get("yaml_settings", {})

    def read_catalog(self) -> CatalogResults | None:
        """Read the catalog file if it exists."""
        logger.debug(":mag: Checking if catalog is already loaded => %s", bool(self._catalog))
        if not self._catalog:
            catalog = _load_catalog(self.settings)
            if not catalog and self.settings.create_catalog_if_not_exists:
                logger.info(
                    ":bookmark_tabs: No existing catalog found, generating new catalog.json."
                )
                catalog = _generate_catalog(self.project)
            self._catalog = catalog
        return self._catalog

    def __post_init__(self) -> None:
        logger.debug(":green_book: Running post-init for YamlRefactorContext.")
        if EMPTY_STRING not in self.placeholders:
            self.placeholders = (EMPTY_STRING, *self.placeholders)
        for setting, val in self.yaml_settings.items():
            setattr(self.yaml_handler, setting, val)
        self.pool._max_workers = self.project.runtime_cfg.threads
        logger.info(
            ":notebook: Osmosis ThreadPoolExecutor max_workers synced with dbt => %s",
            self.pool._max_workers,
        )


def _load_catalog(settings: YamlRefactorSettings) -> CatalogResults | None:
    """Load the catalog file if it exists and return a CatalogResults instance."""
    logger.debug(":mag: Attempting to load catalog from => %s", settings.catalog_path)
    if not settings.catalog_path:
        return None
    fp = Path(settings.catalog_path)
    if not fp.exists():
        logger.warning(":warning: Catalog path => %s does not exist.", fp)
        return None
    logger.info(":books: Loading existing catalog => %s", fp)
    return t.cast(CatalogResults, CatalogArtifact.from_dict(json.loads(fp.read_text())))


# NOTE: this is mostly adapted from dbt-core with some cruft removed, strict pyright is not a fan of dbt's shenanigans
def _generate_catalog(context: DbtProjectContext) -> CatalogResults | None:
    """Generate the dbt catalog file for the project."""
    if context.config.disable_introspection:
        logger.warning(":warning: Introspection is disabled, cannot generate catalog.")
        return
    logger.info(
        ":books: Generating a new catalog for the project => %s", context.runtime_cfg.project_name
    )
    catalogable_nodes = chain(
        [
            t.cast(RelationConfig, node)  # pyright: ignore[reportInvalidCast]
            for node in context.manifest.nodes.values()
            if node.is_relational and not node.is_ephemeral_model
        ],
        [t.cast(RelationConfig, node) for node in context.manifest.sources.values()],  # pyright: ignore[reportInvalidCast]
    )
    table, exceptions = context.adapter.get_filtered_catalog(
        catalogable_nodes,
        context.manifest.get_used_schemas(),  # pyright: ignore[reportArgumentType]
    )

    logger.debug(":mag_right: Building catalog from returned table => %s", table)
    catalog = Catalog(
        [dict(zip(table.column_names, map(dbt_utils._coerce_decimal, row))) for row in table]  # pyright: ignore[reportUnknownArgumentType,reportPrivateUsage]
    )

    errors: list[str] | None = None
    if exceptions:
        errors = [str(e) for e in exceptions]
        logger.warning(":warning: Exceptions encountered in get_filtered_catalog => %s", errors)

    nodes, sources = catalog.make_unique_id_map(context.manifest)
    artifact = CatalogArtifact.from_results(
        nodes=nodes,
        sources=sources,
        generated_at=datetime.now(timezone.utc),
        compile_results=None,
        errors=errors,
    )
    artifact_path = Path(context.runtime_cfg.project_target_path, "catalog.json")
    logger.info(":bookmark_tabs: Writing fresh catalog => %s", artifact_path)
    artifact.write(str(artifact_path.resolve()))  # Cache it, same as dbt
    return t.cast(CatalogResults, artifact)


# Basic compile & execute
# =======================


def _has_jinja(code: str) -> bool:
    """Check if a code string contains jinja tokens."""
    logger.debug(":crystal_ball: Checking if code snippet has Jinja => %s", code[:50] + "...")
    return any(token in code for token in ("{{", "}}", "{%", "%}", "{#", "#}"))


def compile_sql_code(context: DbtProjectContext, raw_sql: str) -> ManifestSQLNode:
    """Compile jinja SQL using the context's manifest and adapter."""
    logger.info(":zap: Compiling SQL code. Possibly with jinja => %s", raw_sql[:75] + "...")
    tmp_id = str(uuid.uuid4())
    with context.manifest_mutex:
        key = f"{NodeType.SqlOperation}.{context.runtime_cfg.project_name}.{tmp_id}"
        _ = context.manifest.nodes.pop(key, None)

        node = context.sql_parser.parse_remote(raw_sql, tmp_id)
        if not _has_jinja(raw_sql):
            logger.debug(":scroll: No jinja found in the raw SQL, skipping compile steps.")
            return node
        process_node(context.runtime_cfg, context.manifest, node)
        compiled_node = SqlCompileRunner(
            context.runtime_cfg,
            context.adapter,
            node=node,
            node_index=1,
            num_nodes=1,
        ).compile(context.manifest)

        _ = context.manifest.nodes.pop(key, None)

    logger.info(":sparkles: Compilation complete.")
    return compiled_node


def execute_sql_code(context: DbtProjectContext, raw_sql: str) -> tuple[AdapterResponse, Table]:
    """Execute jinja SQL using the context's manifest and adapter."""
    logger.info(":running: Attempting to execute SQL => %s", raw_sql[:75] + "...")
    if _has_jinja(raw_sql):
        comp = compile_sql_code(context, raw_sql)
        sql_to_exec = comp.compiled_code or comp.raw_code
    else:
        sql_to_exec = raw_sql

    resp, table = context.adapter.execute(sql_to_exec, auto_begin=False, fetch=True)
    logger.info(":white_check_mark: SQL execution complete => %s rows returned.", len(table.rows))  # pyright: ignore[reportUnknownArgumentType]
    return resp, table


# Node filtering
# ==============


def _is_fqn_match(node: ResultNode, fqns: list[str]) -> bool:
    """Filter models based on the provided fully qualified name matching on partial segments."""
    logger.debug(":mag_right: Checking if node => %s matches any FQNs => %s", node.unique_id, fqns)
    for fqn_str in fqns:
        parts = fqn_str.split(".")
        segment_match = len(node.fqn[1:]) >= len(parts) and all(
            left == right for left, right in zip(parts, node.fqn[1:])
        )
        if segment_match:
            logger.debug(":white_check_mark: FQN matched => %s", fqn_str)
            return True
    return False


def _is_file_match(node: ResultNode, paths: list[Path | str], root: Path | str) -> bool:
    """Check if a node's file path matches any of the provided file paths or names."""
    node_path = Path(root, node.original_file_path).resolve()
    if node.patch_path:
        absolute_patch_path = Path(root, node.patch_path.partition("://")[-1]).resolve()
    yaml_path = absolute_patch_path if absolute_patch_path.exists() else None
    for model_or_dir in paths:
        model_or_dir = Path(model_or_dir).resolve()
        if node.name == model_or_dir.stem:
            logger.debug(":white_check_mark: Name match => %s", model_or_dir)
            return True
        if model_or_dir.is_dir():
            if model_or_dir in node_path.parents or yaml_path and model_or_dir in yaml_path.parents:
                logger.debug(":white_check_mark: Directory path match => %s", model_or_dir)
                return True
        if model_or_dir.is_file():
            if model_or_dir.samefile(node_path) or yaml_path and model_or_dir.samefile(yaml_path):
                logger.debug(":white_check_mark: File path match => %s", model_or_dir)
                return True
    return False


def _topological_sort(
    candidate_nodes: list[tuple[str, ResultNode]],
) -> list[tuple[str, ResultNode]]:
    """
    Perform a topological sort on the given candidate_nodes (uid, node) pairs
    based on their dependencies. If a cycle is detected, raise a ValueError.

    Kahn’s Algorithm:
      1) Build adjacency list: parent -> {child, child, ...}
         (Because if node 'child' depends on 'parent', we have an edge parent->child).
      2) Compute in-degrees for all nodes.
      3) Collect all nodes with in-degree == 0 into a queue.
      4) Repeatedly pop from queue and 'visit' that node,
         then decrement the in-degree of its children.
         If any child's in-degree becomes 0, push it into the queue.
      5) If we visited all nodes, we have a valid topological order.
         Otherwise, a cycle exists.
    """
    adjacency: defaultdict[str, set[str]] = defaultdict(set)
    in_degree: defaultdict[str, int] = defaultdict(int)

    all_uids = set(uid for uid, _ in candidate_nodes)

    for uid, _ in candidate_nodes:
        in_degree[uid] = 0

    for uid, node in candidate_nodes:
        for dep_uid in node.depends_on_nodes:
            if dep_uid in all_uids:
                adjacency[dep_uid].add(uid)
                in_degree[uid] += 1

    queue: deque[str] = deque([uid for uid, deg in in_degree.items() if deg == 0])
    sorted_uids: list[str] = []

    while queue:
        parent_uid = queue.popleft()
        sorted_uids.append(parent_uid)

        for child_uid in adjacency[parent_uid]:
            in_degree[child_uid] -= 1
            if in_degree[child_uid] == 0:
                queue.append(child_uid)

    if len(sorted_uids) < len(candidate_nodes):
        raise ValueError(
            "Cycle detected in node dependencies. Cannot produce a valid topological order."
        )

    uid_to_node = dict(candidate_nodes)
    return [(uid, uid_to_node[uid]) for uid in sorted_uids]


def _iter_candidate_nodes(
    context: YamlRefactorContext,
    include_external: bool = False,
) -> Iterator[tuple[str, ResultNode]]:
    """Iterate over the models in the dbt project manifest applying the filter settings."""
    logger.debug(
        ":mag: Filtering nodes (models/sources/seeds) with user-specified settings => %s",
        context.settings,
    )

    def f(node: ResultNode, include_external: bool = False) -> bool:
        """Closure to filter models based on the context settings."""
        if node.resource_type not in (NodeType.Model, NodeType.Source, NodeType.Seed):
            return False
        if node.package_name != context.project.runtime_cfg.project_name and not include_external:
            return False
        if node.resource_type == NodeType.Model and node.config.materialized == "ephemeral":
            return False
        if context.settings.models:
            if (
                not _is_file_match(
                    node, context.settings.models, context.project.runtime_cfg.project_root
                )
                and not include_external
            ):
                return False
        if context.settings.fqn:
            if not _is_fqn_match(node, context.settings.fqn):
                return False
        logger.debug(":white_check_mark: Node => %s passed filtering logic.", node.unique_id)
        return True

    candidate_nodes: list[t.Any] = []
    items = chain(context.project.manifest.nodes.items(), context.project.manifest.sources.items())
    for uid, dbt_node in items:
        if f(dbt_node, include_external):
            candidate_nodes.append((uid, dbt_node))

    for uid, node in _topological_sort(candidate_nodes):
        yield uid, node


# Introspection
# =============


@t.overload
def _find_first(coll: Iterable[T], predicate: t.Callable[[T], bool], default: T) -> T: ...


@t.overload
def _find_first(
    coll: Iterable[T], predicate: t.Callable[[T], bool], default: None = ...
) -> T | None: ...


def _find_first(
    coll: Iterable[T], predicate: t.Callable[[T], bool], default: T | None = None
) -> T | None:
    """Find the first item in a container that satisfies a predicate."""
    for item in coll:
        if predicate(item):
            return item
    return default


def normalize_column_name(column: str, credentials_type: str) -> str:
    """Apply case normalization to a column name based on the credentials type."""
    if credentials_type == "snowflake" and column.startswith('"') and column.endswith('"'):
        logger.debug(":snowflake: Column name found with double-quotes => %s", column)
        pass
    elif credentials_type == "snowflake":
        return column.upper()
    return column.strip('"').strip("`").strip("[]")


def _maybe_use_precise_dtype(
    col: BaseColumn, settings: YamlRefactorSettings, node: ResultNode | None = None
) -> str:
    """Use the precise data type if enabled in the settings."""
    use_num_prec = _get_setting_for_node(
        "numeric-precision-and-scale", node, col.name, fallback=settings.numeric_precision_and_scale
    )
    use_chr_prec = _get_setting_for_node(
        "string-length", node, col.name, fallback=settings.string_length
    )
    if (col.is_numeric() and use_num_prec) or (col.is_string() and use_chr_prec):
        logger.debug(":ruler: Using precise data type => %s", col.data_type)
        return col.data_type
    if hasattr(col, "mode"):
        return col.data_type
    return col.dtype


def get_table_ref(node: ResultNode | BaseRelation) -> TableRef:
    """Make an appropriate table ref for a dbt node or relation."""
    if isinstance(node, BaseRelation):
        assert node.schema, "Schema must be set for a BaseRelation to generate a TableRef"
        assert node.identifier, "Identifier must be set for a BaseRelation to generate a TableRef"
        return TableRef(node.database, node.schema, node.identifier)
    elif node.resource_type == NodeType.Source:
        return TableRef(node.database, node.schema, node.identifier or node.name)
    else:
        return TableRef(node.database, node.schema, node.alias or node.name)


_COLUMN_LIST_CACHE = {}
"""Cache for column lists to avoid redundant introspection."""


def get_columns(context: YamlRefactorContext, ref: TableRef) -> dict[str, ColumnMetadata]:
    """Equivalent to get_columns_meta in old code but directly referencing a key, not a node."""
    if ref in _COLUMN_LIST_CACHE:
        logger.debug(":blue_book: Column list cache HIT => %s", ref)
        return _COLUMN_LIST_CACHE[ref]

    logger.info(":mag_right: Collecting columns for table => %s", ref)
    normalized_cols = OrderedDict()
    offset = 0

    def process_column(c: BaseColumn | ColumnMetadata, /) -> None:
        nonlocal offset

        cols = [c]
        if hasattr(c, "flatten"):
            cols.extend(getattr(c, "flatten")())

        for col in cols:
            if any(re.match(b, col.name) for b in context.ignore_patterns):
                logger.debug(
                    ":no_entry_sign: Skipping column => %s due to skip pattern match.", col.name
                )
                return
            normalized = normalize_column_name(
                col.name, context.project.runtime_cfg.credentials.type
            )
            if not isinstance(col, ColumnMetadata):
                dtype = _maybe_use_precise_dtype(col, context.settings)
                col = ColumnMetadata(
                    name=normalized, type=dtype, index=offset, comment=getattr(col, "comment", None)
                )
            normalized_cols[normalized] = col
            offset += 1

    if catalog := context.read_catalog():
        logger.debug(":blue_book: Catalog found => Checking for ref => %s", ref)
        catalog_entry = _find_first(
            chain(catalog.nodes.values(), catalog.sources.values()), lambda c: c.key() == ref
        )
        if catalog_entry:
            logger.info(":books: Found catalog entry for => %s. Using it to process columns.", ref)
            for column in catalog_entry.columns.values():
                process_column(column)
            return normalized_cols

    if context.project.config.disable_introspection:
        logger.warning(
            ":warning: Introspection is disabled, cannot introspect columns and no catalog entry."
        )
        return normalized_cols

    relation: BaseRelation | None = context.project.adapter.get_relation(*ref)
    if relation is None:
        logger.warning(":warning: No relation found => %s", ref)
        return normalized_cols

    try:
        logger.info(":mag: Introspecting columns in warehouse for => %s", relation)
        for column in t.cast(
            Iterable[BaseColumn], context.project.adapter.get_columns_in_relation(relation)
        ):
            process_column(column)
    except Exception as ex:
        logger.warning(":warning: Could not introspect columns for %s: %s", ref, ex)

    _COLUMN_LIST_CACHE[ref] = normalized_cols
    return normalized_cols


# TODO: instead of getting specific keys, perhaps we get a NodeConfigContext object scoped to a node / node+column
# and internally the __getitem__ or similar handles the complex resolution of keys (under the hood, we can
# probably use a ChainMap)
def _get_setting_for_node(
    opt: str,
    /,
    node: ResultNode | None = None,
    col: str | None = None,
    *,
    fallback: t.Any | None = None,
) -> t.Any:
    """Get a configuration value for a dbt node from the node's meta and config.

    models: # dbt_project
      project:
        staging:
          +dbt-osmosis: path/spec.yml
          +dbt-osmosis-options:
            string-length: true
            numeric-precision-and-scale: true
            skip-add-columns: true
          +dbt-osmosis-skip-add-tags: true

    models: # schema
      - name: foo
        meta:
          string-length: false
          prefix: user_ # we strip this prefix to inherit from columns upstream, useful in staging models that prefix everything
        columns:
          - bar:
            meta:
              dbt-osmosis-skip-meta-merge: true # per-column options
              dbt-osmosis-options:
                output-to-lower: true

    {{ config(..., dbt_osmosis_options={"prefix": "account_"}) }} -- sql

    We check for
    From node column meta
    - <key>
    - dbt-osmosis-<key>
    - dbt-osmosis-options.<key>
    From node meta
    - <key>
    - dbt-osmosis-<key>
    - dbt-osmosis-options.<key>
    From node config
    - dbt-osmosis-<key>
    - dbt-osmosis-options.<key>
    - dbt_osmosis_<key> # allows use in {{ config(...) }} by being a valid python identifier
    - dbt_osmosis_options.<key> # allows use in {{ config(...) }} by being a valid python identifier
    """
    if node is None:
        return fallback
    k, identifier = opt.replace("_", "-"), opt.replace("-", "_")
    sources = [
        node.meta,
        node.meta.get("dbt-osmosis-options", {}),
        node.meta.get("dbt_osmosis_options", {}),
        node.config.extra,
        node.config.extra.get("dbt-osmosis-options", {}),
        node.config.extra.get("dbt_osmosis_options", {}),
    ]
    if col and (column := node.columns.get(col)):
        sources = [
            column.meta,
            column.meta.get("dbt-osmosis-options", {}),
            column.meta.get("dbt_osmosis_options", {}),
            *sources,
        ]
    for source in sources:
        for variation in (f"dbt-osmosis-{k}", f"dbt_osmosis_{identifier}"):
            if variation in source:
                return source[variation]
        if source is not node.config.extra:
            if k in source:
                return source[k]
            if identifier in source:
                return source[identifier]
    return fallback


# Restructuring Logic
# ===================


def create_missing_source_yamls(context: YamlRefactorContext) -> None:
    """Create source files for sources defined in the dbt_project.yml dbt-osmosis var which don't exist as nodes.

    This is a useful preprocessing step to ensure that all sources are represented in the dbt project manifest. We
    do not have rich node information for non-existent sources, hence the alternative codepath here to bootstrap them.
    """
    if context.project.config.disable_introspection:
        logger.warning(":warning: Introspection is disabled, cannot create missing source YAMLs.")
        return
    logger.info(":factory: Creating missing source YAMLs (if any).")
    database: str = context.project.runtime_cfg.credentials.database
    lowercase: bool = context.settings.output_to_lower

    did_side_effect: bool = False
    for source, spec in context.source_definitions.items():
        if isinstance(spec, str):
            schema = source
            src_yaml_path = spec
        elif isinstance(spec, dict):
            database = t.cast(str, spec.get("database", database))
            schema = t.cast(str, spec.get("schema", source))
            src_yaml_path = t.cast(str, spec["path"])
        else:
            continue

        if _find_first(
            context.project.manifest.sources.values(), lambda s: s.source_name == source
        ):
            logger.debug(
                ":white_check_mark: Source => %s already exists in the manifest, skipping creation.",
                source,
            )
            continue

        src_yaml_path = Path(
            context.project.runtime_cfg.project_root,
            context.project.runtime_cfg.model_paths[0],
            src_yaml_path.lstrip(os.sep),
        )

        def _describe(rel: BaseRelation) -> dict[str, t.Any]:
            s = {
                "name": rel.identifier,
                "description": "",
                "columns": [
                    {
                        "name": name.lower() if lowercase else name,
                        "description": meta.comment or "",
                        "data_type": meta.type.lower() if lowercase else meta.type,
                    }
                    for name, meta in get_columns(context, get_table_ref(rel)).items()
                ],
            }
            if context.settings.skip_add_data_types:
                for col in t.cast(list[dict[str, t.Any]], s["columns"]):
                    _ = col.pop("data_type", None)
            return s

        tables = [
            schema
            for schema in context.pool.map(
                _describe,
                context.project.adapter.list_relations(database=database, schema=schema),
            )
        ]
        source = {"name": source, "database": database, "schema": schema, "tables": tables}

        src_yaml_path.parent.mkdir(parents=True, exist_ok=True)
        with src_yaml_path.open("w") as f:
            logger.info(":books: Injecting new source => %s => %s", source["name"], src_yaml_path)
            context.yaml_handler.dump({"version": 2, "sources": [source]}, f)
            context.register_mutations(1)

        did_side_effect = True

    if did_side_effect:
        logger.info(
            ":arrows_counterclockwise: Some new sources were created, reloading the project."
        )
        _reload_manifest(context.project)


class MissingOsmosisConfig(Exception):
    """Raised when an osmosis configuration is missing."""


def _get_yaml_path_template(context: YamlRefactorContext, node: ResultNode) -> str | None:
    """Get the yaml path template for a dbt model or source node."""
    if node.resource_type == NodeType.Source:
        def_or_path = context.source_definitions.get(node.source_name)
        if isinstance(def_or_path, dict):
            return def_or_path.get("path")
        return def_or_path
    conf = [
        c.get(k)
        for k in ("dbt-osmosis", "dbt_osmosis")
        for c in (node.config.extra, node.unrendered_config)
    ]
    path_template = _find_first(t.cast(list[t.Union[str, None]], conf), lambda v: v is not None)
    if not path_template:
        raise MissingOsmosisConfig(
            f"Config key `dbt-osmosis: <path>` not set for model {node.name}"
        )
    logger.debug(":gear: Resolved YAML path template => %s", path_template)
    return path_template


def get_current_yaml_path(context: YamlRefactorContext, node: ResultNode) -> Path | None:
    """Get the current yaml path for a dbt model or source node."""
    if node.resource_type in (NodeType.Model, NodeType.Seed) and getattr(node, "patch_path", None):
        path = Path(context.project.runtime_cfg.project_root).joinpath(
            t.cast(str, node.patch_path).partition("://")[-1]
        )
        logger.debug(":page_facing_up: Current YAML path => %s", path)
        return path
    if node.resource_type == NodeType.Source:
        path = Path(context.project.runtime_cfg.project_root, node.path)
        logger.debug(":page_facing_up: Current YAML path => %s", path)
        return path
    return None


def get_target_yaml_path(context: YamlRefactorContext, node: ResultNode) -> Path:
    """Get the target yaml path for a dbt model or source node."""
    tpl = _get_yaml_path_template(context, node)
    if not tpl:
        logger.warning(":warning: No path template found for => %s", node.unique_id)
        return Path(context.project.runtime_cfg.project_root, node.original_file_path)

    rendered = tpl.format(node=node, model=node.name, parent=node.fqn[-2])
    segments: list[Path | str] = []

    if node.resource_type == NodeType.Source:
        segments.append(context.project.runtime_cfg.model_paths[0])
    elif rendered.startswith("/"):
        segments.append(context.project.runtime_cfg.model_paths[0])
        rendered = rendered.lstrip("/")
    else:
        segments.append(Path(node.original_file_path).parent)

    if not (rendered.endswith(".yml") or rendered.endswith(".yaml")):
        rendered += ".yml"
    segments.append(rendered)

    path = Path(context.project.runtime_cfg.project_root, *segments)
    logger.debug(":star2: Target YAML path => %s", path)
    return path


def build_yaml_file_mapping(
    context: YamlRefactorContext, create_missing_sources: bool = False
) -> dict[str, SchemaFileLocation]:
    """Build a mapping of dbt model and source nodes to their current and target yaml paths."""
    logger.info(
        ":globe_with_meridians: Building YAML file mapping. create_missing_sources => %s",
        create_missing_sources,
    )

    if create_missing_sources:
        create_missing_source_yamls(context)

    out_map: dict[str, SchemaFileLocation] = {}
    for uid, node in _iter_candidate_nodes(context):
        current_path = get_current_yaml_path(context, node)
        out_map[uid] = SchemaFileLocation(
            target=get_target_yaml_path(context, node).resolve(),
            current=current_path.resolve() if current_path else None,
            node_type=node.resource_type,
        )

    logger.debug(":card_index_dividers: Built YAML file mapping => %s", out_map)
    return out_map


_YAML_BUFFER_CACHE: dict[Path, t.Any] = {}
"""Cache for yaml file buffers to avoid redundant disk reads/writes and simplify edits."""


def _read_yaml(context: YamlRefactorContext, path: Path) -> dict[str, t.Any]:
    """Read a yaml file from disk. Adds an entry to the buffer cache so all operations on a path are consistent."""
    with context.yaml_handler_lock:
        if path not in _YAML_BUFFER_CACHE:
            if not path.is_file():
                logger.debug(":warning: Path => %s is not a file. Returning empty doc.", path)
                return _YAML_BUFFER_CACHE.setdefault(path, {})
            logger.debug(":open_file_folder: Reading YAML doc => %s", path)
            _YAML_BUFFER_CACHE[path] = t.cast(dict[str, t.Any], context.yaml_handler.load(path))
    return _YAML_BUFFER_CACHE[path]


def _write_yaml(context: YamlRefactorContext, path: Path, data: dict[str, t.Any]) -> None:
    """Write a yaml file to disk and register a mutation with the context. Clears the path from the buffer cache."""
    logger.debug(":page_with_curl: Attempting to write YAML to => %s", path)
    if not context.settings.dry_run:
        with context.yaml_handler_lock:
            path.parent.mkdir(parents=True, exist_ok=True)
            original = path.read_bytes() if path.is_file() else b""
            context.yaml_handler.dump(data, staging := io.BytesIO())
            modified = staging.getvalue()
            if modified != original:
                logger.info(":writing_hand: Writing changes to => %s", path)
                with path.open("wb") as f:
                    _ = f.write(modified)
                    context.register_mutations(1)
            else:
                logger.debug(":white_check_mark: Skipping write => %s (no changes)", path)
            del staging
            if path in _YAML_BUFFER_CACHE:
                del _YAML_BUFFER_CACHE[path]


def commit_yamls(context: YamlRefactorContext) -> None:
    """Commit all files in the yaml buffer cache to disk. Clears the buffer cache and registers mutations."""
    logger.info(":inbox_tray: Committing all YAMLs from buffer cache to disk.")
    if not context.settings.dry_run:
        with context.yaml_handler_lock:
            for path in list(_YAML_BUFFER_CACHE.keys()):
                original = path.read_bytes() if path.is_file() else b""
                context.yaml_handler.dump(_YAML_BUFFER_CACHE[path], staging := io.BytesIO())
                modified = staging.getvalue()
                if modified != original:
                    logger.info(":writing_hand: Writing => %s", path)
                    with path.open("wb") as f:
                        logger.info(f"Writing {path}")
                        _ = f.write(modified)
                        context.register_mutations(1)
                else:
                    logger.debug(":white_check_mark: Skipping => %s (no changes)", path)
                del _YAML_BUFFER_CACHE[path]


def _generate_minimal_model_yaml(node: ModelNode | SeedNode) -> dict[str, t.Any]:
    """Generate a minimal model yaml for a dbt model node."""
    logger.debug(":baby: Generating minimal yaml for Model/Seed => %s", node.name)
    return {"name": node.name, "columns": []}


def _generate_minimal_source_yaml(node: SourceDefinition) -> dict[str, t.Any]:
    """Generate a minimal source yaml for a dbt source node."""
    logger.debug(":baby: Generating minimal yaml for Source => %s", node.name)
    return {"name": node.source_name, "tables": [{"name": node.name, "columns": []}]}


def _create_operations_for_node(
    context: YamlRefactorContext, uid: str, loc: SchemaFileLocation
) -> list[RestructureOperation]:
    """Create restructure operations for a dbt model or source node."""
    logger.debug(":bricks: Creating restructure operations for => %s", uid)
    node = context.project.manifest.nodes.get(uid) or context.project.manifest.sources.get(uid)
    if not node:
        logger.warning(":warning: Node => %s not found in manifest.", uid)
        return []

    # If loc.current is None => we are generating a brand new file
    # If loc.current => we unify it with the new location
    ops: list[RestructureOperation] = []

    if loc.current is None:
        logger.info(":sparkles: No current YAML file, building minimal doc => %s", uid)
        if isinstance(node, (ModelNode, SeedNode)):
            minimal = _generate_minimal_model_yaml(node)
            ops.append(
                RestructureOperation(
                    file_path=loc.target,
                    content={"version": 2, f"{node.resource_type}s": [minimal]},
                )
            )
        else:
            minimal = _generate_minimal_source_yaml(t.cast(SourceDefinition, node))
            ops.append(
                RestructureOperation(
                    file_path=loc.target,
                    content={"version": 2, "sources": [minimal]},
                )
            )
    else:
        existing = _read_yaml(context, loc.current)
        injectable: dict[str, t.Any] = {"version": 2}
        injectable.setdefault("models", [])
        injectable.setdefault("sources", [])
        injectable.setdefault("seeds", [])
        if loc.node_type == NodeType.Model:
            assert isinstance(node, ModelNode)
            for obj in existing.get("models", []):
                if obj["name"] == node.name:
                    injectable["models"].append(obj)
                    break
        elif loc.node_type == NodeType.Source:
            assert isinstance(node, SourceDefinition)
            for src in existing.get("sources", []):
                if src["name"] == node.source_name:
                    injectable["sources"].append(src)
                    break
        elif loc.node_type == NodeType.Seed:
            assert isinstance(node, SeedNode)
            for seed in existing.get("seeds", []):
                if seed["name"] == node.name:
                    injectable["seeds"].append(seed)
        ops.append(
            RestructureOperation(
                file_path=loc.target,
                content=injectable,
                superseded_paths={loc.current: [node]},
            )
        )
    return ops


def draft_restructure_delta_plan(context: YamlRefactorContext) -> RestructureDeltaPlan:
    """Draft a restructure plan for the dbt project."""
    logger.info(":bulb: Drafting restructure delta plan for the project.")
    plan = RestructureDeltaPlan()
    lock = threading.Lock()

    def _job(uid: str, loc: SchemaFileLocation) -> None:
        ops = _create_operations_for_node(context, uid, loc)
        with lock:
            plan.operations.extend(ops)

    futs: list[Future[None]] = []
    for uid, loc in build_yaml_file_mapping(context).items():
        if not loc.is_valid:
            futs.append(context.pool.submit(_job, uid, loc))
    done, _ = wait(futs, return_when=FIRST_EXCEPTION)
    for fut in done:
        exc = fut.exception()
        if exc:
            logger.error(":bomb: Error encountered while drafting plan => %s", exc)
            raise exc
    logger.info(":star2: Draft plan creation complete => %s operations", len(plan.operations))
    return plan


def pretty_print_plan(plan: RestructureDeltaPlan) -> None:
    """Pretty print the restructure plan for the dbt project."""
    logger.info(":mega: Restructure plan includes => %s operations.", len(plan.operations))
    for op in plan.operations:
        str_content = str(op.content)[:80] + "..."
        logger.info(":sparkles: Processing => %s", str_content)
        if not op.superseded_paths:
            logger.info(":blue_book: CREATE or MERGE => %s", op.file_path)
        else:
            old_paths = [p.name for p in op.superseded_paths.keys()] or ["UNKNOWN"]
            logger.info(":blue_book: %s -> %s", old_paths, op.file_path)


def _remove_models(existing_doc: dict[str, t.Any], nodes: list[ResultNode]) -> None:
    """Clean up the existing yaml doc by removing models superseded by the restructure plan."""
    logger.debug(":scissors: Removing superseded models => %s", [n.name for n in nodes])
    to_remove = {n.name for n in nodes if n.resource_type == NodeType.Model}
    keep = []
    for section in existing_doc.get("models", []):
        if section.get("name") not in to_remove:
            keep.append(section)
    existing_doc["models"] = keep


def _remove_seeds(existing_doc: dict[str, t.Any], nodes: list[ResultNode]) -> None:
    """Clean up the existing yaml doc by removing models superseded by the restructure plan."""
    logger.debug(":scissors: Removing superseded seeds => %s", [n.name for n in nodes])
    to_remove = {n.name for n in nodes if n.resource_type == NodeType.Seed}
    keep = []
    for section in existing_doc.get("seeds", []):
        if section.get("name") not in to_remove:
            keep.append(section)
    existing_doc["seeds"] = keep


def _remove_sources(existing_doc: dict[str, t.Any], nodes: list[ResultNode]) -> None:
    """Clean up the existing yaml doc by removing sources superseded by the restructure plan."""
    to_remove_sources = {
        (n.source_name, n.name) for n in nodes if n.resource_type == NodeType.Source
    }
    logger.debug(":scissors: Removing superseded sources => %s", sorted(to_remove_sources))
    keep_sources = []
    for section in existing_doc.get("sources", []):
        keep_tables = []
        for tbl in section.get("tables", []):
            if (section["name"], tbl["name"]) not in to_remove_sources:
                keep_tables.append(tbl)
        if keep_tables:
            section["tables"] = keep_tables
            keep_sources.append(section)
    existing_doc["sources"] = keep_sources


def _sync_doc_section(
    context: YamlRefactorContext, node: ResultNode, doc_section: dict[str, t.Any]
) -> None:
    """Helper function that overwrites 'doc_section' with data from 'node'.

    This includes columns, description, meta, tags, etc.
    We assume node is the single source of truth, so doc_section is replaced.
    """
    logger.debug(":arrows_counterclockwise: Syncing doc_section with node => %s", node.unique_id)
    if node.description and not doc_section.get("description"):
        doc_section["description"] = node.description

    current_columns: list[dict[str, t.Any]] = doc_section.setdefault("columns", [])
    incoming_columns: list[dict[str, t.Any]] = []

    current_map = {}
    for c in current_columns:
        norm_name = normalize_column_name(c["name"], context.project.runtime_cfg.credentials.type)
        current_map[norm_name] = c

    for name, meta in node.columns.items():
        cdict = meta.to_dict()
        cdict["name"] = name
        norm_name = normalize_column_name(name, context.project.runtime_cfg.credentials.type)

        current_yaml = t.cast(dict[str, t.Any], current_map.get(norm_name, {}))
        merged = dict(current_yaml)

        skip_add_types = _get_setting_for_node(
            "skip-add-data-types", node, name, fallback=context.settings.skip_add_data_types
        )
        for k, v in cdict.items():
            if k == "description" and not v:
                merged.pop("description", None)
            elif k == "data_type" and skip_add_types and merged.get("data_type") is None:
                pass
            else:
                merged[k] = v

        if not merged.get("description"):
            merged.pop("description", None)
        if merged.get("tags") == []:
            merged.pop("tags", None)
        if merged.get("meta") == {}:
            merged.pop("meta", None)

        for k in list(merged.keys()):
            if not merged[k]:
                merged.pop(k)

        if _get_setting_for_node(
            "output-to-lower", node, name, fallback=context.settings.output_to_lower
        ):
            merged["name"] = merged["name"].lower()

        incoming_columns.append(merged)

    doc_section["columns"] = incoming_columns


def sync_node_to_yaml(
    context: YamlRefactorContext, node: ResultNode | None = None, *, commit: bool = True
) -> None:
    """Synchronize a single node's columns, description, tags, meta, etc. from the manifest into its corresponding YAML file.

    We assume the manifest node is the single source of truth, so the YAML file is overwritten to match.

    - If the YAML file doesn't exist yet, we create it with minimal structure.
    - If the YAML file exists, we read it from the file/ cache, locate the node's section,
      and then overwrite that section to match the node's current columns, meta, etc.

    This is a one-way sync:
        Manifest Node => YAML

    All changes to the Node (columns, metadata, etc.) should happen before calling this function.
    """
    if node is None:
        logger.info(":wave: No single node specified; synchronizing all matched nodes.")
        for _ in context.pool.map(
            partial(sync_node_to_yaml, context, commit=commit),
            (n for _, n in _iter_candidate_nodes(context)),
        ):
            ...
        return

    current_path = get_current_yaml_path(context, node)
    if not current_path or not current_path.exists():
        logger.debug(
            ":warning: Current path does not exist => %s. Using target path instead.", current_path
        )
        current_path = get_target_yaml_path(context, node)

    doc: dict[str, t.Any] = _read_yaml(context, current_path)
    if not doc:
        doc = {"version": 2}

    if node.resource_type == NodeType.Source:
        resource_k = "sources"
    elif node.resource_type == NodeType.Seed:
        resource_k = "seeds"
    else:
        resource_k = "models"

    if node.resource_type == NodeType.Source:
        # The doc structure => sources: [ { "name": <source_name>, "tables": [...]}, ... ]
        # Step A: find or create the source
        doc_source: dict[str, t.Any] | None = None
        for s in doc.setdefault(resource_k, []):
            if s.get("name") == node.source_name:
                doc_source = s
                break
        if not doc_source:
            doc_source = {
                "name": node.source_name,
                "tables": [],
            }
            doc["sources"].append(doc_source)

        # Step B: find or create the table
        doc_table: dict[str, t.Any] | None = None
        for t_ in doc_source["tables"]:
            if t_.get("name") == node.name:
                doc_table = t_
                break
        if not doc_table:
            doc_table = {
                "name": node.name,
                "columns": [],
            }
            doc_source["tables"].append(doc_table)

        # We'll store the columns & description on "doc_table"
        # For source, "description" is stored at table-level in the Node
        _sync_doc_section(context, node, doc_table)

    else:
        # Models or Seeds => doc[ "models" ] or doc[ "seeds" ] is a list of { "name", "description", "columns", ... }
        doc_list = doc.setdefault(resource_k, [])
        doc_obj: dict[str, t.Any] | None = None
        for item in doc_list:
            if item.get("name") == node.name:
                # check if model is versioned
                # versions have to be defined in yaml meaning
                # there'll always be an existing properties yaml file
                if isinstance(node, ModelNode) and node.version is not None:
                    for version in item.get("versions", []):
                        if version.get("v") == node.version:
                            doc_obj = version
                            break
                else:
                    doc_obj = item
                    break
        if not doc_obj:
            doc_obj = {
                "name": node.name,
                "columns": [],
            }
            doc_list.append(doc_obj)

        _sync_doc_section(context, node, doc_obj)

    for k in ("models", "sources", "seeds"):
        if len(doc.get(k, [])) == 0:
            _ = doc.pop(k, None)

    if commit:
        logger.info(":inbox_tray: Committing YAML doc changes for => %s", node.unique_id)
        _write_yaml(context, current_path, doc)


def apply_restructure_plan(
    context: YamlRefactorContext, plan: RestructureDeltaPlan, *, confirm: bool = False
) -> None:
    """Apply the restructure plan for the dbt project."""
    if not plan.operations:
        logger.info(":white_check_mark: No changes needed in the restructure plan.")
        return

    if confirm:
        logger.info(":warning: Confirm option set => printing plan and waiting for user input.")
        pretty_print_plan(plan)

    while confirm:
        response = input("Apply the restructure plan? [y/N]: ")
        if response.lower() in ("y", "yes"):
            break
        elif response.lower() in ("n", "no", ""):
            logger.info("Skipping restructure plan.")
            return
        logger.warning(":loudspeaker: Please respond with 'y' or 'n'.")

    for op in plan.operations:
        logger.debug(":arrow_right: Applying restructure operation => %s", op)
        output_doc: dict[str, t.Any] = {"version": 2}
        if op.file_path.exists():
            existing_data = _read_yaml(context, op.file_path)
            output_doc.update(existing_data)

        for key, val in op.content.items():
            if isinstance(val, list):
                output_doc.setdefault(key, []).extend(val)
            elif isinstance(val, dict):
                output_doc.setdefault(key, {}).update(val)
            else:
                output_doc[key] = val

        _write_yaml(context, op.file_path, output_doc)

        for path, nodes in op.superseded_paths.items():
            if path.is_file():
                existing_data = _read_yaml(context, path)

                if "models" in existing_data:
                    _remove_models(existing_data, nodes)
                if "sources" in existing_data:
                    _remove_sources(existing_data, nodes)
                if "seeds" in existing_data:
                    _remove_seeds(existing_data, nodes)

                keys = set(existing_data.keys()) - {"version"}
                if all(len(existing_data.get(k, [])) == 0 for k in keys):
                    if not context.settings.dry_run:
                        path.unlink(missing_ok=True)
                        if path.parent.exists() and not any(path.parent.iterdir()):
                            path.parent.rmdir()
                        if path in _YAML_BUFFER_CACHE:
                            del _YAML_BUFFER_CACHE[path]
                    context.register_mutations(1)
                    logger.info(":heavy_minus_sign: Superseded entire file => %s", path)
                else:
                    _write_yaml(context, path, existing_data)
                    logger.info(
                        ":arrow_forward: Migrated doc from => %s to => %s", path, op.file_path
                    )

    logger.info(
        ":arrows_counterclockwise: Committing all restructure changes and reloading manifest."
    )
    _ = commit_yamls(context), _reload_manifest(context.project)


# Inheritance Logic
# =================


def _build_node_ancestor_tree(
    manifest: Manifest,
    node: ResultNode,
    tree: dict[str, list[str]] | None = None,
    visited: set[str] | None = None,
    depth: int = 1,
) -> dict[str, list[str]]:
    """Build a flat graph of a node and it's ancestors."""
    logger.debug(":seedling: Building ancestor tree/branch for => %s", node.unique_id)
    if tree is None or visited is None:
        visited = set(node.unique_id)
        tree = {"generation_0": [node.unique_id]}
        depth = 1

    if not hasattr(node, "depends_on"):
        return tree

    for dep in getattr(node.depends_on, "nodes", []):
        if not dep.startswith(("model.", "seed.", "source.")):
            continue
        if dep not in visited:
            visited.add(dep)
            member = manifest.nodes.get(dep, manifest.sources.get(dep))
            if member:
                tree.setdefault(f"generation_{depth}", []).append(dep)
                _ = _build_node_ancestor_tree(manifest, member, tree, visited, depth + 1)

    for generation in tree.values():
        generation.sort()  # For deterministic ordering

    return tree


def _get_node_yaml(
    context: YamlRefactorContext, member: ResultNode
) -> MappingProxyType[str, t.Any] | None:
    """Get a read-only view of the parsed YAML for a dbt model or source node."""
    project_dir = Path(context.project.runtime_cfg.project_root)

    if isinstance(member, SourceDefinition):
        if not member.original_file_path:
            return None
        path = project_dir.joinpath(member.original_file_path)
        sources = t.cast(list[dict[str, t.Any]], _read_yaml(context, path).get("sources", []))
        source = _find_first(sources, lambda s: s["name"] == member.source_name, {})
        tables = source.get("tables", [])
        maybe_doc = _find_first(tables, lambda tbl: tbl["name"] == member.name)
        if maybe_doc is not None:
            return MappingProxyType(maybe_doc)

    elif isinstance(member, (ModelNode, SeedNode)):
        if not member.patch_path:
            return None
        path = project_dir.joinpath(member.patch_path.split("://")[-1])
        section = f"{member.resource_type}s"
        models = t.cast(list[dict[str, t.Any]], _read_yaml(context, path).get(section, []))
        maybe_doc = _find_first(models, lambda model: model["name"] == member.name)
        if maybe_doc is not None:
            return MappingProxyType(maybe_doc)

    return None


def _build_column_knowledge_graph(
    context: YamlRefactorContext, node: ResultNode
) -> dict[str, dict[str, t.Any]]:
    """Generate a column knowledge graph for a dbt model or source node."""
    tree = _build_node_ancestor_tree(context.project.manifest, node)
    logger.debug(":family_tree: Node ancestor tree => %s", tree)

    pm = get_plugin_manager()
    node_column_variants: dict[str, list[str]] = {}
    for column_name, _ in node.columns.items():
        variants = node_column_variants.setdefault(column_name, [column_name])
        for v in pm.hook.get_candidates(name=column_name, node=node, context=context.project):
            variants.extend(t.cast(list[str], v))

    def _get_unrendered(k: str, ancestor: ResultNode) -> t.Any:
        raw_yaml = _get_node_yaml(context, ancestor) or {}
        raw_columns = t.cast(list[dict[str, t.Any]], raw_yaml.get("columns", []))
        raw_column_metadata = _find_first(
            raw_columns,
            lambda c: normalize_column_name(c["name"], context.project.runtime_cfg.credentials.type)
            in node_column_variants[name],
            {},
        )
        return raw_column_metadata.get(k)

    column_knowledge_graph: dict[str, dict[str, t.Any]] = {}
    for generation in reversed(sorted(tree.keys())):
        ancestors = tree[generation]
        for ancestor_uid in ancestors:
            ancestor = context.project.manifest.nodes.get(
                ancestor_uid, context.project.manifest.sources.get(ancestor_uid)
            )
            if not isinstance(ancestor, (SourceDefinition, SeedNode, ModelNode)):
                continue

            for name, _ in node.columns.items():
                graph_node = column_knowledge_graph.setdefault(name, {})
                for variant in node_column_variants[name]:
                    incoming = ancestor.columns.get(variant)
                    if incoming is not None:
                        break
                else:
                    continue
                graph_edge = incoming.to_dict()

                if _get_setting_for_node(
                    "add-progenitor-to-meta",
                    node,
                    name,
                    fallback=context.settings.add_progenitor_to_meta,
                ):
                    graph_node.setdefault("meta", {}).setdefault(
                        "osmosis_progenitor", ancestor.unique_id
                    )

                if _get_setting_for_node(
                    "use-unrendered-descriptions",
                    node,
                    name,
                    fallback=context.settings.use_unrendered_descriptions,
                ):
                    if unrendered_description := _get_unrendered("description", ancestor):
                        graph_edge["description"] = unrendered_description

                current_tags = graph_node.get("tags", [])
                if merged_tags := (set(graph_edge.pop("tags", [])) | set(current_tags)):
                    graph_edge["tags"] = list(merged_tags)

                current_meta = graph_node.get("meta", {})
                if merged_meta := {**current_meta, **graph_edge.pop("meta", {})}:
                    graph_edge["meta"] = merged_meta

                for inheritable in _get_setting_for_node(
                    "add-inheritance-for-specified-keys",
                    node,
                    name,
                    fallback=context.settings.add_inheritance_for_specified_keys,
                ):
                    current_val = graph_node.get(inheritable)
                    if incoming_unrendered_val := _get_unrendered(inheritable, ancestor):
                        graph_edge[inheritable] = incoming_unrendered_val
                    elif incoming_val := graph_edge.pop(inheritable, current_val):
                        graph_edge[inheritable] = incoming_val

                if graph_edge.get("description", EMPTY_STRING) in context.placeholders or (
                    generation == "generation_0"
                    and _get_setting_for_node(
                        "force_inherit_descriptions",
                        node,
                        name,
                        fallback=context.settings.force_inherit_descriptions,
                    )
                ):
                    _ = graph_edge.pop("description", None)
                if graph_edge.get("tags") == []:
                    del graph_edge["tags"]
                if graph_edge.get("meta") == {}:
                    del graph_edge["meta"]
                for k in list(graph_edge.keys()):
                    if graph_edge[k] is None:
                        graph_edge.pop(k)

                graph_node.update(graph_edge)

    return column_knowledge_graph


# Operations
# ==========


@dataclass
class TransformOperation:
    """An operation to be run on a dbt manifest node."""

    func: t.Callable[..., t.Any]
    name: str

    _result: t.Any | None = field(init=False, default=None)
    _context: YamlRefactorContext | None = field(init=False, default=None)
    _node: ResultNode | None = field(init=False, default=None)
    _metadata: dict[str, t.Any] = field(init=False, default_factory=dict)

    @property
    def result(self) -> t.Any:
        """The result of the operation or None."""
        return self._result

    @property
    def metadata(self) -> MappingProxyType[str, t.Any]:
        """Metadata about the operation."""
        return MappingProxyType(self._metadata)

    def __call__(
        self, context: YamlRefactorContext, node: ResultNode | None = None
    ) -> TransformOperation:
        """Run the operation and store the result."""
        self._context = context
        self._node = node
        self._metadata["started"] = True
        try:
            self.func(context, node)
            self._metadata["success"] = True
        except Exception as e:
            self._metadata["error"] = str(e)
            raise
        return self

    def __rshift__(self, next_op: TransformOperation) -> TransformPipeline:
        """Chain operations together."""
        return TransformPipeline([self]) >> next_op

    def __repr__(self) -> str:  # pyright: ignore[reportImplicitOverride]
        return f"<Operation: {self.name} (success={self.metadata.get('success', False)})>"


@dataclass
class TransformPipeline:
    """A pipeline of transform operations to be run on a dbt manifest node."""

    operations: list[TransformOperation] = field(default_factory=list)
    commit_mode: t.Literal["none", "batch", "atomic", "defer"] = "batch"

    _metadata: dict[str, t.Any] = field(init=False, default_factory=dict)

    @property
    def metadata(self) -> MappingProxyType[str, t.Any]:
        """Metadata about the pipeline."""
        return MappingProxyType(self._metadata)

    def __rshift__(self, next_op: TransformOperation | t.Callable[..., t.Any]) -> TransformPipeline:
        """Chain operations together."""
        if isinstance(next_op, TransformOperation):
            self.operations.append(next_op)
        elif callable(next_op):
            self.operations.append(TransformOperation(next_op, next_op.__name__))
        else:
            raise ValueError(f"Cannot chain non-callable: {next_op}")  # pyright: ignore[reportUnreachable]
        return self

    def __call__(
        self, context: YamlRefactorContext, node: ResultNode | None = None
    ) -> TransformPipeline:
        """Run all operations in the pipeline."""
        logger.info(
            "\n:gear: [b]Running pipeline[/b] with => %s operations %s \n",
            len(self.operations),
            [op.name for op in self.operations],
        )

        self._metadata["started_at"] = (pipeline_start := time.time())
        for op in self.operations:
            logger.info(
                ":gear:  [b]Starting to[/b] [yellow]%s[/yellow]",
                op.name,
            )
            step_start = time.time()
            _ = op(context, node)
            step_end = time.time()
            logger.info(
                ":sparkles: [b]Done with[/b] [green]%s[/green] in %.2fs \n",
                op.name,
                step_end - step_start,
            )
            self._metadata.setdefault("steps", []).append({
                **op.metadata,
                "duration": step_end - step_start,
            })
            if self.commit_mode == "atomic":
                logger.info(
                    ":hourglass: [b]Committing[/b] Operation => [green]%s[/green]",
                    op.name,
                )
                sync_node_to_yaml(context, node, commit=True)
                logger.info(":checkered_flag: [b]Committed[/b] \n")
        self._metadata["completed_at"] = (pipeline_end := time.time())

        logger.info(
            ":checkered_flag: [b]Manifest transformation pipeline [green]completed[/green] in => %.2fs[/b]",
            pipeline_end - pipeline_start,
        )

        def _commit() -> None:
            logger.info(":hourglass: Committing all changes to YAML files in batch.")
            _commit_start = time.time()
            sync_node_to_yaml(context, node, commit=True)
            _commit_end = time.time()
            logger.info(
                ":checkered_flag: YAML commits completed in => %.2fs", _commit_end - _commit_start
            )

        if self.commit_mode == "batch":
            _commit()
        elif self.commit_mode == "defer":
            _ = atexit.register(_commit)

        return self

    def __repr__(self) -> str:  # pyright: ignore[reportImplicitOverride]
        steps = [op.name for op in self.operations]
        return f"<OperationPipeline: {len(self.operations)} operations, steps={steps!r}>"


def _transform_op(
    name: str | None = None,
) -> t.Callable[[t.Callable[[YamlRefactorContext, ResultNode | None], None]], TransformOperation]:
    """Decorator to create a TransformOperation from a function."""

    def decorator(
        func: t.Callable[[YamlRefactorContext, ResultNode | None], None],
    ) -> TransformOperation:
        return TransformOperation(func, name=name or func.__name__)

    return decorator


@_transform_op("Inherit Upstream Column Knowledge")
def inherit_upstream_column_knowledge(
    context: YamlRefactorContext, node: ResultNode | None = None
) -> None:
    """Inherit column level knowledge from the ancestors of a dbt model or source node."""
    if node is None:
        logger.info(":wave: Inheriting column knowledge across all matched nodes.")
        for _ in context.pool.map(
            partial(inherit_upstream_column_knowledge, context),
            (n for _, n in _iter_candidate_nodes(context, include_external=True)),
        ):
            ...
        return

    logger.info(":dna: Inheriting column knowledge for => %s", node.unique_id)

    column_knowledge_graph = _build_column_knowledge_graph(context, node)
    kwargs = None
    for name, node_column in node.columns.items():
        kwargs = column_knowledge_graph.get(name)
        if kwargs is None:
            continue
        inheritable = ["description"]
        if not _get_setting_for_node(
            "skip-add-tags", node, name, fallback=context.settings.skip_add_tags
        ):
            inheritable.append("tags")
        if not _get_setting_for_node(
            "skip-merge-meta", node, name, fallback=context.settings.skip_merge_meta
        ):
            inheritable.append("meta")
        for extra in _get_setting_for_node(
            "add-inheritance-for-specified-keys",
            node,
            name,
            fallback=context.settings.add_inheritance_for_specified_keys,
        ):
            if extra not in inheritable:
                inheritable.append(extra)

        updated_metadata = {k: v for k, v in kwargs.items() if v is not None and k in inheritable}
        logger.debug(
            ":star2: Inheriting updated metadata => %s for column => %s", updated_metadata, name
        )
        node.columns[name] = node_column.replace(**updated_metadata)


@_transform_op("Inject Missing Columns")
def inject_missing_columns(context: YamlRefactorContext, node: ResultNode | None = None) -> None:
    """Add missing columns to a dbt node and it's corresponding yaml section. Changes are implicitly buffered until commit_yamls is called."""
    if _get_setting_for_node("skip-add-columns", node, fallback=context.settings.skip_add_columns):
        logger.debug(":no_entry_sign: Skipping column injection (skip_add_columns=True).")
        return
    if node is None:
        logger.info(":wave: Injecting missing columns for all matched nodes.")
        for _ in context.pool.map(
            partial(inject_missing_columns, context),
            (n for _, n in _iter_candidate_nodes(context)),
        ):
            ...
        return
    if (
        _get_setting_for_node(
            "skip-add-source-columns", node, fallback=context.settings.skip_add_source_columns
        )
        and node.resource_type == NodeType.Source
    ):
        logger.debug(":no_entry_sign: Skipping column injection (skip_add_source_columns=True).")
        return
    current_columns = {
        normalize_column_name(c.name, context.project.runtime_cfg.credentials.type)
        for c in node.columns.values()
    }
    incoming_columns = get_columns(context, get_table_ref(node))
    for incoming_name, incoming_meta in incoming_columns.items():
        if incoming_name not in node.columns and incoming_name not in current_columns:
            logger.info(
                ":heavy_plus_sign: Reconciling missing column => %s in node => %s",
                incoming_name,
                node.unique_id,
            )
            gen_col = {"name": incoming_name, "description": incoming_meta.comment or ""}
            if (dtype := incoming_meta.type) and not _get_setting_for_node(
                "skip-add-data-types", node, fallback=context.settings.skip_add_data_types
            ):
                gen_col["data_type"] = dtype.lower() if context.settings.output_to_lower else dtype
            node.columns[incoming_name] = ColumnInfo.from_dict(gen_col)


@_transform_op("Remove Extra Columns")
def remove_columns_not_in_database(
    context: YamlRefactorContext, node: ResultNode | None = None
) -> None:
    """Remove columns from a dbt node and it's corresponding yaml section that are not present in the database. Changes are implicitly buffered until commit_yamls is called."""
    if node is None:
        logger.info(":wave: Removing columns not in DB across all matched nodes.")
        for _ in context.pool.map(
            partial(remove_columns_not_in_database, context),
            (n for _, n in _iter_candidate_nodes(context)),
        ):
            ...
        return
    current_columns = {
        normalize_column_name(c.name, context.project.runtime_cfg.credentials.type)
        for c in node.columns.values()
    }
    incoming_columns = get_columns(context, get_table_ref(node))
    if not incoming_columns:
        logger.info(
            ":no_entry_sign: No columns discovered for node => %s, skipping cleanup.",
            node.unique_id,
        )
        return
    extra_columns = current_columns - set(incoming_columns.keys())
    for extra_column in extra_columns:
        logger.info(
            ":heavy_minus_sign: Removing extra column => %s in node => %s",
            extra_column.lower(),
            node.unique_id,
        )
        _ = node.columns.pop(extra_column.lower(), None)


@_transform_op("Sort Columns in DB Order")
def sort_columns_as_in_database(
    context: YamlRefactorContext, node: ResultNode | None = None
) -> None:
    """Sort columns in a dbt node and it's corresponding yaml section as they appear in the database. Changes are implicitly buffered until commit_yamls is called."""
    if node is None:
        logger.info(":wave: Sorting columns as they appear in DB across all matched nodes.")
        for _ in context.pool.map(
            partial(sort_columns_as_in_database, context),
            (n for _, n in _iter_candidate_nodes(context)),
        ):
            ...
        return
    logger.info(":1234: Sorting columns by warehouse order => %s", node.unique_id)
    incoming_columns = get_columns(context, get_table_ref(node))
    if not incoming_columns:
        logger.info(
            ":no_entry_sign: No columns discovered for node => %s, skipping db order sorting.",
            node.unique_id,
        )
        return

    def _position(column: dict[str, t.Any]):
        db_info = incoming_columns.get(column["name"].upper())
        if db_info is None:
            return 99999
        return db_info.index

    node.columns = {
        k: v for k, v in sorted(node.columns.items(), key=lambda i: _position(i[1].to_dict()))
    }


@_transform_op("Sort Columns Alphabetically")
def sort_columns_alphabetically(
    context: YamlRefactorContext, node: ResultNode | None = None
) -> None:
    """Sort columns in a dbt node and it's corresponding yaml section alphabetically. Changes are implicitly buffered until commit_yamls is called."""
    if node is None:
        logger.info(":wave: Sorting columns alphabetically across all matched nodes.")
        for _ in context.pool.map(
            partial(sort_columns_alphabetically, context),
            (n for _, n in _iter_candidate_nodes(context)),
        ):
            ...
        return
    logger.info(":abcd: Sorting columns alphabetically => %s", node.unique_id)
    node.columns = {k: v for k, v in sorted(node.columns.items(), key=lambda i: i[0])}


@_transform_op("Sort Columns")
def sort_columns_as_configured(
    context: YamlRefactorContext, node: ResultNode | None = None
) -> None:
    if node is None:
        logger.info(":wave: Sorting columns as configured across all matched nodes.")
        for _ in context.pool.map(
            partial(sort_columns_as_configured, context),
            (n for _, n in _iter_candidate_nodes(context)),
        ):
            ...
        return
    sort_by = _get_setting_for_node("sort-by", node, fallback="database")
    if sort_by == "database":
        _ = sort_columns_as_in_database(context, node)
    elif sort_by == "alphabetical":
        _ = sort_columns_alphabetically(context, node)
    else:
        raise ValueError(f"Invalid sort-by value: {sort_by} for node: {node.unique_id}")


@_transform_op("Synchronize Data Types")
def synchronize_data_types(context: YamlRefactorContext, node: ResultNode | None = None) -> None:
    """Populate data types for columns in a dbt node and it's corresponding yaml section. Changes are implicitly buffered until commit_yamls is called."""
    if node is None:
        logger.info(":wave: Populating data types across all matched nodes.")
        for _ in context.pool.map(
            partial(synchronize_data_types, context), (n for _, n in _iter_candidate_nodes(context))
        ):
            ...
        return
    logger.info(":1234: Synchronizing data types => %s", node.unique_id)
    incoming_columns = get_columns(context, get_table_ref(node))
    if _get_setting_for_node("skip-add-data-types", node, fallback=False):
        return
    for name, column in node.columns.items():
        if _get_setting_for_node(
            "skip-add-data-types", node, name, fallback=context.settings.skip_add_data_types
        ):
            continue
        lowercase = _get_setting_for_node(
            "output-to-lower", node, name, fallback=context.settings.output_to_lower
        )
        if inc_c := incoming_columns.get(name.upper()):
            is_lower = column.data_type and column.data_type.islower()
            if inc_c.type:
                inc_c_type = inc_c.type.replace("character varying", "varchar").replace("varchar(16777216)", "varchar")
                column.data_type = inc_c_type.lower() if lowercase or is_lower else inc_c_type


@_transform_op("Synthesize Missing Documentation")
def synthesize_missing_documentation_with_openai(
    context: YamlRefactorContext, node: ResultNode | None = None
) -> None:
    """Synthesize missing documentation for a dbt node using OpenAI's GPT-4o API."""
    try:
        from dbt_osmosis.core.llm import (
            generate_column_doc,
            generate_model_spec_as_json,
            generate_table_doc,
        )
    except ImportError:
        raise ImportError("Please install the 'dbt-osmosis[openai]' extra to use this feature.")
    if node is None:
        logger.info(":wave: Synthesizing missing documentation across all matched nodes.")
        for _ in context.pool.map(
            partial(synthesize_missing_documentation_with_openai, context),
            (n for _, n in _iter_candidate_nodes(context)),
        ):
            ...
        return
    # since we are topologically sorted, we continually pass down synthesized knowledge leveraging our inheritance system
    # which minimizes synthesis requests -- in some cases by an order of magnitude while increasing accuracy
    _ = inherit_upstream_column_knowledge(context, node)
    total = len(node.columns)
    if total == 0:
        logger.info(
            ":no_entry_sign: No columns to synthesize documentation for => %s", node.unique_id
        )
        return
    documented = len([
        column
        for column in node.columns.values()
        if column.description and column.description not in context.placeholders
    ])
    node_map = ChainMap(
        t.cast(dict[str, ResultNode], context.project.manifest.nodes),
        t.cast(dict[str, ResultNode], context.project.manifest.sources),
    )
    upstream_docs: list[str] = ["# The following is not exhaustive, but provides some context."]
    depends_on_nodes = t.cast(list[str], node.depends_on_nodes)
    for i, uid in enumerate(depends_on_nodes):
        dep = node_map.get(uid)
        if dep is not None:
            oneline_desc = dep.description.replace("\n", " ")
            upstream_docs.append(f"{uid}: # {oneline_desc}")
            for j, (name, meta) in enumerate(dep.columns.items()):
                if meta.description and meta.description not in context.placeholders:
                    upstream_docs.append(f"- {name}: |\n{textwrap.indent(meta.description, '  ')}")
                if j > 20:
                    # just a small amount of this supplementary context is sufficient
                    upstream_docs.append("- (omitting additional columns for brevity)")
                    break
        # ensure our context window is bounded, semi-arbitrary
        if len(upstream_docs) > 100 and i < len(depends_on_nodes) - 1:
            upstream_docs.append(f"# remaining nodes are: {', '.join(depends_on_nodes[i:])}")
            break
    if len(upstream_docs) == 1:
        upstream_docs[0] = "(no upstream documentation found)"
    if (
        total - documented
        > 10  # a semi-arbitrary limit by which its probably better to one shot the table versus many smaller requests
    ):
        logger.info(
            ":robot: Synthesizing bulk documentation for => %s columns in node => %s",
            total - documented,
            node.unique_id,
        )
        spec = generate_model_spec_as_json(
            getattr(
                node, "raw_code", f"SELECT {', '.join(node.columns)} FROM {node.schema}.{node.name}"
            ),
            upstream_docs=upstream_docs,
            existing_context=f"NodeId={node.unique_id}\nTableDescription={node.description}",
            temperature=0.4,
        )
        if not node.description or node.description in context.placeholders:
            node.description = spec.get("description", node.description)
        for synth_col in spec.get("columns", []):
            usr_col = node.columns.get(synth_col["name"])
            if usr_col and (not usr_col.description or usr_col.description in context.placeholders):
                usr_col.description = synth_col.get("description", usr_col.description)
    else:
        if not node.description or node.description in context.placeholders:
            logger.info(
                ":robot: Synthesizing documentation for node => %s",
                node.unique_id,
            )
            node.description = generate_table_doc(
                getattr(
                    node,
                    "raw_code",
                    f"SELECT {', '.join(node.columns)} FROM {node.schema}.{node.name}",
                ),
                table_name=node.relation_name or node.name,
                upstream_docs=upstream_docs,
            )
        for column_name, column in node.columns.items():
            if not column.description or column.description in context.placeholders:
                logger.info(
                    ":robot: Synthesizing documentation for column => %s in node => %s",
                    column_name,
                    node.unique_id,
                )
                column.description = generate_column_doc(
                    column_name,
                    existing_context=f"DataType={column.data_type or 'unknown'}>\nColumnParent={node.unique_id}\nTableDescription={node.description}",
                    table_name=node.relation_name or node.name,
                    upstream_docs=upstream_docs,
                    temperature=0.7,
                )


# Fuzzy Plugins
# =============

_hookspec = pluggy.HookspecMarker("dbt-osmosis")
hookimpl = pluggy.HookimplMarker("dbt-osmosis")


@_hookspec
def get_candidates(name: str, node: ResultNode, context: DbtProjectContext) -> list[str]:  # pyright: ignore[reportUnusedParameter]
    """Get a list of candidate names for a column."""
    raise NotImplementedError


class FuzzyCaseMatching:
    @hookimpl
    def get_candidates(self, name: str, node: ResultNode, context: DbtProjectContext) -> list[str]:
        """Get a list of candidate names for a column based on case variants."""
        _ = node, context
        variants = [
            name.lower(),  # lowercase
            name.upper(),  # UPPERCASE
            cc := re.sub("_(.)", lambda m: m.group(1).upper(), name),  # camelCase
            cc[0].upper() + cc[1:],  # PascalCase
        ]
        logger.debug(":lower_upper_case: FuzzyCaseMatching variants => %s", variants)
        return variants


class FuzzyPrefixMatching:
    @hookimpl
    def get_candidates(self, name: str, node: ResultNode, context: DbtProjectContext) -> list[str]:
        """Get a list of candidate names for a column excluding a prefix."""
        _ = context
        variants = []
        p = _get_setting_for_node("prefix", node, name)
        if p:
            mut_name = name.removeprefix(p)
            logger.debug(
                ":scissors: FuzzyPrefixMatching => removing prefix '%s' => %s", p, mut_name
            )
            variants.append(mut_name)
        return variants


@lru_cache(maxsize=None)
def get_plugin_manager():
    """Get the pluggy plugin manager for dbt-osmosis."""
    manager = pluggy.PluginManager("dbt-osmosis")
    _ = manager.register(FuzzyCaseMatching())
    _ = manager.register(FuzzyPrefixMatching())
    _ = manager.load_setuptools_entrypoints("dbt-osmosis")
    return manager


if __name__ == "__main__":
    # Kitchen sink
    c = DbtConfiguration(
        project_dir="demo_duckdb", profiles_dir="demo_duckdb", vars={"dbt-osmosis": {}}
    )

    project = create_dbt_project_context(c)
    _ = _generate_catalog(project)

    yaml_context = YamlRefactorContext(
        project, settings=YamlRefactorSettings(use_unrendered_descriptions=True)
    )

    create_missing_source_yamls(yaml_context)

    plan = draft_restructure_delta_plan(yaml_context)
    apply_restructure_plan(yaml_context, plan, confirm=True)

    pipeline = (
        inject_missing_columns
        >> remove_columns_not_in_database
        >> inherit_upstream_column_knowledge
        >> sort_columns_as_configured
        >> synchronize_data_types
    )
    pipeline.commit_mode = "atomic"
    _ = pipeline(context=yaml_context)
